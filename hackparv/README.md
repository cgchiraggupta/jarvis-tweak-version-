# Self-Operating Computer + Assistant Integration

ğŸ¤– **AI-Powered Computer Control on macOS**

This project integrates two powerful open-source frameworks to enable AI-driven computer automation:
- **[self-operating-computer](https://github.com/OthersideAI/self-operating-computer)** - Python framework for multimodal computer control
- **Assistant** - Node.js conversational AI API with GPT-4 Vision

## âœ¨ Features

- ğŸ–±ï¸ **Automated Mouse Control** - AI clicks, drags, and navigates
- âŒ¨ï¸ **Keyboard Automation** - Types text and executes shortcuts
- ğŸ‘€ **Visual Understanding** - Analyzes screen state using GPT-4 Vision
- ğŸ¯ **Goal-Oriented** - Completes tasks step-by-step
- ğŸ”„ **Multi-Model Support** - Easily switch between AI providers
- ğŸ¤ **Voice Commands** - Optional voice input support
- ğŸŒ **Web Automation** - Open apps, browse web, perform searches

## ğŸ¬ Quick Start

### 1. Install & Configure (5 minutes)

```bash
# Install Python dependencies
cd self-operating-computer
pip install -r requirements.txt

# Install Node.js dependencies
cd ../jarvis/server
npm install

# Configure OpenAI API key
cp config.example .env
nano .env  # Add your OpenAI API key
```

### 2. Grant macOS Permissions

Go to **System Settings** â†’ **Privacy & Security** and add **Terminal** to:
- Screen Recording
- Accessibility

### 3. Start the Assistant API

```bash
# From project root
./start_assistant.sh
```

### 4. Run Your First Command

```bash
cd self-operating-computer
operate --model=assistant --prompt="open Safari"
```

**ğŸ“– See [QUICKSTART.md](QUICKSTART.md) for detailed setup instructions.**

## ğŸ“š Documentation

| Document | Description |
|----------|-------------|
| **[QUICKSTART.md](QUICKSTART.md)** | Get started in 5 minutes |
| **[INTEGRATION_README.md](INTEGRATION_README.md)** | Complete integration documentation |
| **[USAGE_GUIDE.md](USAGE_GUIDE.md)** | Usage patterns and examples |
| **[examples/](examples/)** | Example scripts and workflows |

## ğŸ¯ Example Commands

```bash
# Open applications
operate --model=assistant --prompt="open VS Code"
operate --model=assistant --prompt="launch Terminal"

# Web browsing
operate --model=assistant --prompt="open Safari and go to github.com"
operate --model=assistant --prompt="search Google for Python tutorials"

# File management
operate --model=assistant --prompt="open Finder and navigate to Documents"

# Complex workflows
operate --model=assistant --prompt="open Safari, search for weather, and show forecast"
```

## ğŸ—ï¸ How It Works

```
User Command â†’ Screenshot â†’ GPT-4 Vision â†’ Actions â†’ Execute â†’ Repeat
```

1. **Capture**: Takes a screenshot of your desktop
2. **Analyze**: Sends to GPT-4 Vision for analysis
3. **Plan**: AI determines next action to achieve goal
4. **Execute**: Performs mouse/keyboard actions
5. **Loop**: Repeats until objective complete

## ğŸ› ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Input (Text/Voice/GUI)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  self-operating-computer (Python)           â”‚
â”‚  â€¢ Captures screenshots                     â”‚
â”‚  â€¢ Sends to Assistant API                   â”‚
â”‚  â€¢ Executes actions                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ HTTP
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Assistant API (Node.js)                    â”‚
â”‚  â€¢ HTTP Server (port 4001)                  â”‚
â”‚  â€¢ WebSocket Server (port 4000)             â”‚
â”‚  â€¢ GPT-4 Vision integration                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OpenAI GPT-4 Vision                        â”‚
â”‚  â€¢ Analyzes screenshots                     â”‚
â”‚  â€¢ Generates action plans                   â”‚
â”‚  â€¢ Returns JSON instructions                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Project Structure

```
hackparv/
â”œâ”€â”€ self-operating-computer/      # Python computer control framework
â”‚   â”œâ”€â”€ operate/
â”‚   â”‚   â”œâ”€â”€ main.py              # Entry point
â”‚   â”‚   â”œâ”€â”€ operate.py           # Main orchestration
â”‚   â”‚   â”œâ”€â”€ config.py            # Configuration
â”‚   â”‚   â””â”€â”€ models/
â”‚   â”‚       â”œâ”€â”€ apis.py          # Model integrations
â”‚   â”‚       â””â”€â”€ assistant_adapter.py  # âœ¨ NEW: Assistant integration
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ jarvis/                      # Node.js Assistant API
â”‚   â””â”€â”€ server/
â”‚       â”œâ”€â”€ index.js             # WebSocket server
â”‚       â”œâ”€â”€ http_server.js       # âœ¨ NEW: HTTP API
â”‚       â”œâ”€â”€ actions.js           # Action handlers
â”‚       â””â”€â”€ package.json         # âœ¨ UPDATED: Dependencies
â”‚
â”œâ”€â”€ examples/                    # âœ¨ NEW: Example scripts
â”‚   â”œâ”€â”€ example_workflows.sh     # Interactive menu
â”‚   â”œâ”€â”€ example_api_usage.py     # Direct API usage
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ start_assistant.sh           # âœ¨ NEW: Server startup script
â”œâ”€â”€ test_integration.py          # âœ¨ NEW: Integration tests
â”‚
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ QUICKSTART.md               # âœ¨ NEW: 5-minute setup guide
â”œâ”€â”€ INTEGRATION_README.md       # âœ¨ NEW: Complete documentation
â””â”€â”€ USAGE_GUIDE.md              # âœ¨ NEW: Usage patterns & examples
```

## ğŸ”§ Configuration

### Environment Variables

**Python (self-operating-computer)**:
```bash
export ASSISTANT_API_URL=http://localhost:4001  # Optional, defaults to localhost:4001
```

**Node.js (jarvis/server/.env)**:
```env
KEY=your_openai_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
HTTP_PORT=4001
WEBSOCKET_PORT=4000
```

## ğŸ§ª Testing

Run the integration test suite:

```bash
python3 test_integration.py
```

This checks:
- âœ… Python dependencies
- âœ… Assistant API connectivity
- âœ… API endpoints
- âœ… macOS permissions

## ğŸ® Command-Line Options

```bash
operate --model=assistant [OPTIONS]

Options:
  -m, --model MODEL       AI model to use (assistant, gpt-4-with-ocr, claude-3, etc.)
  --prompt PROMPT         Direct command (skips interactive prompt)
  --voice                 Enable voice input mode
  --verbose               Show detailed logs
```

## ğŸ”„ Supported Models

Switch between AI providers easily:

```bash
# Use Assistant (this integration)
operate --model=assistant

# Use GPT-4 directly
operate --model=gpt-4-with-ocr

# Use Claude 3
operate --model=claude-3

# Use local Ollama
operate --model=llava
```

## ğŸš€ Advanced Usage

### Interactive Mode
```bash
operate --model=assistant
# Prompts for your command
```

### Voice Mode (requires whisper_mic)
```bash
pip install -r requirements-audio.txt
operate --model=assistant --voice
```

### Verbose Mode (debugging)
```bash
operate --model=assistant --verbose --prompt="your command"
```

### Programmatic Usage
```python
import requests
import base64

# Capture screenshot
with open('screenshot.png', 'rb') as f:
    img_base64 = base64.b64encode(f.read()).decode('utf-8')

# Call Assistant API
response = requests.post('http://localhost:4001/analyze', json={
    'image': img_base64,
    'objective': 'open Safari',
    'prompt': 'Analyze this screen and tell me what to do'
})

# Get operations
operations = response.json()['operations']
```

## ğŸ› Troubleshooting

| Problem | Solution |
|---------|----------|
| Can't connect to API | Run `./start_assistant.sh` to start servers |
| Permission denied | Grant Screen Recording + Accessibility in System Settings |
| Import errors | Run `pip install -r requirements.txt` |
| Invalid API key | Check `.env` file in `jarvis/server/` |

**ğŸ“– See [INTEGRATION_README.md](INTEGRATION_README.md#-troubleshooting) for detailed troubleshooting.**

## ğŸ“Š API Endpoints

The Assistant API provides:

- `GET /health` - Health check
- `POST /analyze` - Analyze screenshot and return actions
- `POST /query` - Simple text query

**ğŸ“– See [INTEGRATION_README.md](INTEGRATION_README.md#-api-endpoints) for API documentation.**

## ğŸ”’ Security

- âœ… API keys stored in `.env` (gitignored)
- âœ… Local-first: Servers run on localhost
- âœ… Minimal permissions: Only Screen Recording + Accessibility
- âœ… Transparent: Verbose mode shows all actions

## ğŸ¤ Contributing

Contributions welcome! To add features:

1. Fork the repository
2. Create a feature branch
3. Make changes
4. Test with `test_integration.py`
5. Submit pull request

## ğŸ“„ License

This integration maintains the licenses of both parent projects:
- **self-operating-computer**: MIT License
- **Assistant**: [Your License]

## ğŸ™ Credits

- **[OthersideAI](https://github.com/OthersideAI)** - self-operating-computer framework
- **[OpenAI](https://openai.com)** - GPT-4 Vision API
- **Open Source Community** - Various dependencies and tools

## ğŸ“§ Support

1. Check documentation in this repository
2. Run `test_integration.py` for diagnostics
3. Enable `--verbose` mode for detailed logs
4. Check [INTEGRATION_README.md](INTEGRATION_README.md) troubleshooting section

## ğŸ¯ Use Cases

- ğŸ§ª **Testing**: Automate UI testing workflows
- ğŸ“Š **Data Entry**: Fill forms and spreadsheets
- ğŸ”„ **Repetitive Tasks**: Automate routine computer operations
- ğŸ“ **Demonstrations**: Create automated demos and tutorials
- ğŸ”¬ **Research**: Study AI-computer interaction
- â™¿ **Accessibility**: Assist users with limited mobility

## ğŸŒŸ Examples in Action

Visit the [examples/](examples/) directory for ready-to-run scripts:

- **example_workflows.sh** - Interactive menu with common tasks
- **example_api_usage.py** - Direct API usage demonstrations
- **More examples** - Check examples/README.md

## ğŸš¦ Getting Started Checklist

- [ ] Install Python dependencies (`pip install -r requirements.txt`)
- [ ] Install Node.js dependencies (`npm install` in jarvis/server)
- [ ] Configure OpenAI API key in `jarvis/server/.env`
- [ ] Grant macOS permissions (Screen Recording + Accessibility)
- [ ] Start Assistant API (`./start_assistant.sh`)
- [ ] Run test suite (`python3 test_integration.py`)
- [ ] Try first command (`operate --model=assistant --prompt="open Safari"`)
- [ ] Read full documentation ([INTEGRATION_README.md](INTEGRATION_README.md))

## ğŸ“ˆ What's Next?

- âœ… Basic integration working
- âœ… HTTP API for vision tasks
- âœ… Documentation and examples
- ğŸ”œ Advanced action types (drag, scroll, etc.)
- ğŸ”œ Error recovery and retries
- ğŸ”œ Context persistence between sessions
- ğŸ”œ GUI for monitoring and control
- ğŸ”œ Pre-built workflows library

---

## ğŸš€ Ready to Start?

```bash
# 1. Start the servers
./start_assistant.sh

# 2. In another terminal, run your first command
cd self-operating-computer
operate --model=assistant --prompt="open Safari"

# 3. Explore examples
cd examples
./example_workflows.sh
```

**ğŸ“– For detailed setup, see [QUICKSTART.md](QUICKSTART.md)**

---

**Built with â¤ï¸ for the AI automation community**

**Star â­ this repo if you find it useful!**




